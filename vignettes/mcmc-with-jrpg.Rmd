---
title: "Custom MCMC Samplers with jrpg"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{mcmc-with-jrpg}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

In this vignette we illustrate how to use Pólya-Gamma latent variables generated by the `jrpg` package to facilitate Gibbs sampling in both logistic regression and negative binomial regression. We will start by loading the following packages:

```{r setup}
library(jrpg)
library(mvtnorm)
```

## Logistic Regression with Pólya-Gamma Latent Variables

The following function may be used to fit a Bayesian logistic regression:
```{r}
logistic_mcmc <- function(X, y, n_iter = 2000, burn_in = 1000) {
  
  # Number of observations and covariates
  N <- nrow(X)
  p <- ncol(X)
  
  # Prior: coefficients have a normal prior N(0, I)
  prior_mean <- rep(0, p)  # Mean vector of the prior
  prior_cov <- diag(1, p)  # Covariance matrix of the prior
  
  # Storage for samples
  beta_samples <- matrix(0, n_iter, p)
  
  # Initialize beta
  beta <- rep(0, p)
  
  b <- rep(1, N) # b = 1 for logistic regression
  
  # MCMC loop
  for (iter in 1:n_iter) {
    
    # Compute linear predictor eta = X %*% beta
    eta <- X %*% beta
    
    # Sample Polya-Gamma latent variables omega_i for i = 1,...,N
    omega <- jrpg(b = b, z = eta)[,1]  

    # Update the posterior covariance and mean for beta
    Omega <- diag(omega)  # N x N diagonal matrix with omega_i
    XtOmegaX <- t(X) %*% Omega %*% X
    post_cov <- solve(XtOmegaX + prior_cov)
    post_mean <- post_cov %*% (t(X) %*% (y - 0.5))
    
    # Sample beta from the posterior distribution N(post_mean, post_cov)
    beta <- rmvnorm(n = 1, mean = post_mean, sigma = post_cov)[1,]
    
    # Store the beta sample
    beta_samples[iter, ] <- beta
    
    if ((iter %% floor(n_iter / 10)) == 0) print(iter)
  }
  
  # Return the beta samples, discarding burn-in samples
  return(beta_samples[(burn_in + 1):n_iter, ])
}
```

The following lines of code are used to simulate a toy dataset with `N = 500` observations.
```{r}
# Example usage of the MCMC sampler
set.seed(123)

# Simulate some data
N <- 500  # Number of observations
p <- 5    # Number of covariates
X <- cbind(1, matrix(rnorm(N * (p - 1)), N, p - 1))  # Design matrix with intercept
beta_true <- c(-1, -0.5, 0, 0.5, 1)  # True coefficients
eta <- X %*% beta_true
prob <- 1 / (1 + exp(-eta))  # Logistic link
y <- rbinom(N, size = 1, prob = prob)  # Binary outcome
```

Now, we run the sampler and summarize the results:
```{r}
# Run the MCMC sampler
beta_samples <- logistic_mcmc(X, y)

# Posterior means of beta
posterior_mean_beta <- colMeans(beta_samples)
print(posterior_mean_beta, digits = 2)
```

Point estimates from the posterior distribution appear to be reasonable.

```{r, out.width='100%', fig.align='center', dpi=300}
# Trace plot
matplot(beta_samples, type = 'l')
abline(h = beta_true, col = 'red', lty = 2)
```

The trace plots indicate we have converged to the posterior distribution.

```{r, out.width='100%', fig.align='center', dpi=300}
# Posterior density plot
beta_post <- apply(beta_samples, 2, \(beta) density(beta)$x)
beta_density <- apply(beta_samples, 2, \(beta) density(beta)$y)
matplot(beta_post, beta_density, type = 'l')
abline(v = beta_true, col = 'red', lty = 2)
```

The above plots demonstrate the true log odds-ratios are captured by the sampler.


## Negative Binomial Regression with Pólya-Gamma Latent Variables

The following function may be used to fit a Bayesian logistic regression:
```{r}
# Negative Binomial MCMC sampler using Polya-Gamma latent variables
nb_mcmc <- function(X, y, r, n_iter = 2000, burn_in = 1000) {
  
  # Number of observations and covariates
  N <- nrow(X)
  p <- ncol(X)
  
  # Prior: coefficients have a normal prior N(0, I)
  prior_mean <- rep(0, p)  # Mean vector of the prior
  prior_cov <- diag(1, p)  # Covariance matrix of the prior
  
  # Storage for samples
  beta_samples <- matrix(0, n_iter, p)
  
  # Initialize beta
  beta <- rep(0, p)
  
  # MCMC loop
  for (iter in 1:n_iter) {
    
    # Compute linear predictor eta = X %*% beta
    eta <- X %*% beta
    
    # Sample Polya-Gamma latent variables omega_i for i = 1,...,N
    # where r is the dispersion parameter in the negative binomial
    omega <- jrpg(b = y + r, z = eta)[,1]
    
    # Update the posterior covariance and mean for beta
    Omega <- diag(omega)  # N x N diagonal matrix with omega_i
    XtOmegaX <- t(X) %*% Omega %*% X
    post_cov <- solve(XtOmegaX + prior_cov)
    post_mean <- post_cov %*% (t(X) %*% ((y - r) / 2))
    
    # Sample beta from the posterior distribution N(post_mean, post_cov)
    beta <- rmvnorm(n = 1, mean = post_mean, sigma = post_cov)[1,]
    
    # Store the beta sample
    beta_samples[iter, ] <- beta
    
    if ((iter %% floor(n_iter / 10)) == 0) print(iter)
  }
  
  # Return the beta samples, discarding burn-in samples
  return(beta_samples[(burn_in + 1):n_iter, ])
}
```

The following lines of code are used to simulate a toy dataset with `N = 500` observations. Generally, the dispersion parameter will not be known and will need a separate updating step in the MCMC (this is usually done with Metroplis-Hastings). However, for simplicity we will assume it is fixed.
```{r}
# Example usage of the MCMC sampler
set.seed(123)

# Simulate some data for Negative Binomial regression
N <- 500  # Number of observations
p <- 5    # Number of covariates
X <- cbind(1, matrix(rnorm(N * (p - 1)), N, p - 1))  # Design matrix with intercept
beta_true <- c(-1, -0.5, 0, 0.5, 1)  # True coefficients
eta <- X %*% beta_true
r_true <- 2  # True dispersion parameter
y <- rnbinom(N, size = r_true, prob = 1 / (1 + exp(eta)))  # Negative Binomial outcome
```

Now, we run the sampler and summarize the results:
```{r}
# Run the MCMC sampler for Negative Binomial regression
beta_samples_nb <- nb_mcmc(X, y, r = r_true)
```

```{r}
# Posterior means of beta
posterior_mean_beta_nb <- colMeans(beta_samples_nb)
print(posterior_mean_beta_nb)
```

Point estimates from the posterior distribution appear to be reasonable.

```{r, out.width='100%', fig.align='center', dpi=300}
# Trace plot
matplot(beta_samples_nb, type = 'l')
abline(h = beta_true, col = 'red', lty = 2)
```

The trace plots indicate we have converged to the posterior distribution.

```{r, out.width='100%', fig.align='center', dpi=300}
# Posterior density plot
beta_post_nb <- apply(beta_samples_nb, 2, \(beta) density(beta)$x)
beta_density_nb <- apply(beta_samples_nb, 2, \(beta) density(beta)$y)
matplot(beta_post_nb, beta_density_nb, type = 'l')
abline(v = beta_true, col = 'red', lty = 2)
```

The above plots demonstrate the true log odds-ratios are captured by the sampler.
